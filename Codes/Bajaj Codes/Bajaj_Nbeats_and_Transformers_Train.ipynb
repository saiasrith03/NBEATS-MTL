{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYp0YJ0FusC6",
    "outputId": "14f479ee-4efa-4319-a2d3-cfd1989319a6"
   },
   "outputs": [],
   "source": [
    "!pip install neuralforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zn3JrhsTvKCZ",
    "outputId": "0df24341-0fc8-45c5-afe1-cd8c172a0e58"
   },
   "outputs": [],
   "source": [
    "!pip install ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2QxyznvxOy",
    "outputId": "d220a4ed-2505-4b9d-da2e-ab1c01de805e"
   },
   "outputs": [],
   "source": [
    "!pip install arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QZsUmkpwH9Z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "plt.style.use('fivethirtyeight') # For plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "XBlH8L6HwgC_",
    "outputId": "a2516fdb-de69-42b9-894c-d6767f7d9a32"
   },
   "outputs": [],
   "source": [
    "bajaj_df = pd.read_csv('bajaj.csv')\n",
    "bajaj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "SqdMg3CNw4P1",
    "outputId": "167961de-140e-4f36-c209-18480f43201c"
   },
   "outputs": [],
   "source": [
    "final_df = bajaj_df.iloc[::-1]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "7ehxiStwxLlp",
    "outputId": "4caceaaa-307d-4334-c4a3-73bec802d9be"
   },
   "outputs": [],
   "source": [
    "df = final_df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "uvHaNwFvxSye",
    "outputId": "374139c3-99a2-4090-9021-9f609cb6845d"
   },
   "outputs": [],
   "source": [
    "df.drop('index',  axis = 1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXWaaZKHxeCB"
   },
   "outputs": [],
   "source": [
    "df = df.set_index('Date')\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Phro4DWNxsPY"
   },
   "outputs": [],
   "source": [
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QxiEmkQxvC_",
    "outputId": "28230c77-f794-42c4-9e10-f1bfe60fa31d"
   },
   "outputs": [],
   "source": [
    "df['Price'] = (df['Price'].str.split()).apply(lambda x: float(x[0].replace(',', '')))\n",
    "df['Open'] = (df['Open'].str.split()).apply(lambda x: float(x[0].replace(',', '')))\n",
    "df['High'] = (df['High'].str.split()).apply(lambda x: float(x[0].replace(',', '')))\n",
    "df['Low'] = (df['Low'].str.split()).apply(lambda x: float(x[0].replace(',', '')))\n",
    "df['Change %'] = (df['Change %'].str.split()).apply(lambda x: float(x[0].replace('%', '')))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "SRDXP5OE48IS",
    "outputId": "67ea3bf5-804e-4ff2-bb29-2cfefe2dd1aa"
   },
   "outputs": [],
   "source": [
    "df = df[['Price', 'Open', 'High', 'Low']].copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Kt-stsQ54_NL",
    "outputId": "d6144448-a96e-427b-83ed-a5de3bcaf383"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5ceagDO1R1l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS\n",
    "from neuralforecast.losses.pytorch import MQLoss, DistributionLoss\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.utils import AirPassengers, AirPassengersPanel, AirPassengersStatic\n",
    "from neuralforecast.models import NBEATSx\n",
    "from neuralforecast.losses.pytorch import MQLoss, DistributionLoss, MSE, MAE, QuantileLoss\n",
    "from pandas.tseries.offsets import CustomBusinessDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPdGcrg11pLu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta as ta\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import t\n",
    "import tensorflow as tf\n",
    "from datetime import date, datetime, timedelta\n",
    "from arch import arch_model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQpE2y2d1qiA"
   },
   "outputs": [],
   "source": [
    "#Return calculation\n",
    "def ReturnCalculation (Database,lag):\n",
    "    dimension=Database.shape[0];dif=lag;Out=np.zeros([dimension-dif])\n",
    "    for i in range(dimension-dif):\n",
    "        Out[i]=(np.log(Database['Price'][i+dif])-np.log(Database['Price'][i]))\n",
    "    return np.append(np.repeat(np.nan, dif),Out), Database.index\n",
    "\n",
    "#STD Calculation\n",
    "def SDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif])\n",
    "    for i in range (dimension-dif):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(np.repeat(np.nan, dif),Out)\n",
    "\n",
    "#STD Calculation\n",
    "def TrueSDCalculation (DailyReturns, LagSD):\n",
    "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif+1])\n",
    "    for i in range (dimension-dif+1):\n",
    "        Out[i]=np.std(DailyReturns[i:i+LagSD],ddof=1)\n",
    "    return np.append(Out,np.repeat(np.nan, dif-1))\n",
    "\n",
    "#Database is calculated\n",
    "def DatabaseGeneration (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index)\n",
    "    return Data.dropna()\n",
    "\n",
    "#Database is calculated\n",
    "def DatabaseGeneration_NBEATS (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'Date': Database.index,'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index)\n",
    "    return Data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_DT4cB41r3B"
   },
   "outputs": [],
   "source": [
    "#MultiHeadSelfAttention\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(query, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(key, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(value, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(concat_attention)  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "\n",
    "#Transformer Keras Block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.nb_dict = {}; self.Bagging=5\n",
    "        for i in range(self.Bagging):\n",
    "          self.nb_dict[\"att{0}\".format(i)]=MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    def call(self, inputs, training):\n",
    "        self.att_dict = {}\n",
    "        for i in range(self.Bagging):\n",
    "          self.att_dict[\"att{0}\".format(i)]=self.nb_dict[\"att{0}\".format(i)](tf.keras.layers.Dropout(.1)(inputs))\n",
    "          if i==0:\n",
    "            self.att_dict[\"attn_output\"]=self.att_dict[\"att{0}\".format(i)]/self.Bagging\n",
    "          else:\n",
    "            self.att_dict[\"attn_output\"]=self.att_dict[\"attn_output\"]+self.att_dict[\"att{0}\".format(i)]/self.Bagging\n",
    "        attn_output = self.dropout1(self.att_dict[\"attn_output\"], training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def Transformer_Model (Shape1, Shape2, HeadsAttention,Dropout, LearningRate):\n",
    "    #Model struture is defined\n",
    "    Input = tf.keras.Input(shape=(Shape1,Shape2), name=\"Input\")\n",
    "    #LSTM is applied on top of the transformer\n",
    "    X = tf.keras.layers.LSTM(units=16, dropout=Dropout, return_sequences=True)(Input)\n",
    "    #Tranformer architecture is implemented\n",
    "    transformer_block_1 = TransformerBlock(embed_dim=16, num_heads=HeadsAttention, ff_dim=8, rate=Dropout)\n",
    "    X = transformer_block_1(X)\n",
    "    #Dense layers are used\n",
    "    X = tf.keras.layers.GlobalAveragePooling1D()(X)\n",
    "    X = tf.keras.layers.Dense(8, activation=tf.nn.sigmoid)(X)\n",
    "    X = tf.keras.layers.Dropout(Dropout)(X)\n",
    "    Output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, name=\"Output\")(X)\n",
    "    model = tf.keras.Model(inputs=Input, outputs=Output)\n",
    "    #Optimizer is defined\n",
    "    Opt = tf.keras.optimizers.Adam(learning_rate=LearningRate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n",
    "    #Model is compiled\n",
    "    model.compile(optimizer=Opt, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vP9LMzsn1thx"
   },
   "outputs": [],
   "source": [
    "#It generates the database for fitting transformer. No positional encoding is needed as LSTM plays this role in the model structure\n",
    "def Transformer_Database (Timestep, XData_AR, YData_AR):\n",
    "    Features = XData_AR.shape[1]; Sample = XData_AR.shape[0]-Timestep+1\n",
    "    XDataTrainScaledRNN=np.zeros([Sample, Timestep, Features]); YDataTrainRNN=np.zeros([Sample])\n",
    "    for i in range(Sample):\n",
    "        XDataTrainScaledRNN[i,:,:] = XData_AR[i:(Timestep+i)]\n",
    "        YDataTrainRNN[i] = YData_AR[Timestep+i-1]\n",
    "    return XDataTrainScaledRNN, YDataTrainRNN\n",
    "\n",
    "#Database is calculated\n",
    "def DatabaseGenerationForecast (Database, Lag, LagSD):\n",
    "    DailyReturns, Index = ReturnCalculation(Database,Lag)\n",
    "    DailyReturnsOld =  np.append(np.repeat(np.nan, 1),DailyReturns[0:(DailyReturns.shape[0]-1)])\n",
    "    SD = SDCalculation (DailyReturns, LagSD)\n",
    "    TrueSD = TrueSDCalculation(DailyReturns, LagSD)\n",
    "    Data = pd.DataFrame({'DailyReturns': DailyReturns, 'SD': SD, 'TrueSD': TrueSD, 'DailyReturnsOld': DailyReturnsOld})\n",
    "    Data = Data.set_index(Index)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UQA0qDI1vA3"
   },
   "outputs": [],
   "source": [
    "#Final AR database for forcasting is generated\n",
    "def DatabaseGenerationForecast_AR (Database, Lag, LagSD, forecast_nbeats):\n",
    "    Data_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).iloc[(-LagSD+1)]\n",
    "    Index_Forecast=DatabaseGenerationForecast(Database, Lag, LagSD).index[(-LagSD+1)]\n",
    "    XDataForecast={'SD': Data_Forecast['SD'], 'DailyReturnsOld': Data_Forecast['DailyReturnsOld'],\n",
    "               'NBEATS-median' : forecast_nbeats}\n",
    "    return pd.DataFrame([XDataForecast], index=[Index_Forecast]), Data_Forecast['DailyReturns']\n",
    "\n",
    "#Transformed ANN-ARCH model forecast\n",
    "def T_ANN_ARCH_Forecast (Database, Lag, LagSD, forecast_nbeats, Scaled_Norm, XData_AR, model):\n",
    "    XDataForecast, ReturnForecast = DatabaseGenerationForecast_AR (Database, Lag, LagSD, forecast_nbeats)\n",
    "    XDataForecast = pd.concat([XData_AR,XDataForecast])\n",
    "    XDataForecastTotalScaled = Scaled_Norm.transform(XDataForecast)\n",
    "    XDataForecastTotalScaled_T, Y_T = Transformer_Database(Timestep, XDataForecastTotalScaled, np.zeros(XDataForecastTotalScaled.shape[0]))\n",
    "    TransformerPrediction = model.predict(XDataForecastTotalScaled_T)\n",
    "    return TransformerPrediction[-1][0], XDataForecast.index[-1], TransformerPrediction[0:(XDataForecastTotalScaled_T.shape[0]-1)], ReturnForecast\n",
    "\n",
    "#It calculates VaR taking into consideration the forecasted sigma to calculate the scale parameter\n",
    "def T_ANN_ARCH_VaR (Alpha, HistoricalReturns, ForecastedSigma, DF):\n",
    "    HistoricalMean = np.mean(HistoricalReturns)\n",
    "    ScaleParameter = np.sqrt((ForecastedSigma**2)*((DF-2)/DF))\n",
    "    VaR = -t.ppf(Alpha, DF, loc=HistoricalMean, scale=ScaleParameter)\n",
    "    return VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guR-eHEl1xfd"
   },
   "outputs": [],
   "source": [
    "def nbeats_model_forecasts(Data, df):\n",
    "  start=Data.index[0].date()\n",
    "  given_date = pd.Timestamp(start)\n",
    "  df = df[df.index < given_date]\n",
    "  df = df.resample('D').fillna(method='ffill')\n",
    "  temp_df = DatabaseGeneration_NBEATS(df, 1, 5)\n",
    "  if temp_df['Date'].iloc[-1] < pd.Timestamp(start-timedelta(days=1)):\n",
    "\n",
    "    new_date_range = pd.date_range(start=temp_df.index.min(), end= start-timedelta(days=1))\n",
    "\n",
    "    # Reindex the DataFrame to the new date range, forward-filling missing values\n",
    "    temp_df = temp_df.reindex(new_date_range).ffill()\n",
    "\n",
    "    # Reset the index if you want 'date' back as a column\n",
    "    temp_df.reset_index(inplace=True)\n",
    "    temp_df = temp_df.drop('Date', axis=1)\n",
    "    temp_df.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "  temp_df['unique_id'] = '1'\n",
    "  temp_df.rename(columns={'Date':'ds'}, inplace=True)\n",
    "  temp_df.rename(columns={'TrueSD':'y'}, inplace=True)\n",
    "\n",
    "  train_df = temp_df[temp_df.ds<temp_df['ds'].values[-500]] # 132 train\n",
    "  test_df = temp_df[temp_df.ds>=temp_df['ds'].values[-500]].reset_index(drop=True)\n",
    "\n",
    "  model = NBEATS(h=len(Data), input_size=1,\n",
    "                loss=MQLoss(level=[90]),\n",
    "                learning_rate= 0.001,\n",
    "                stack_types=['identity', 'identity'],\n",
    "                n_blocks=[1,1],\n",
    "                mlp_units= [[512, 512], [512, 512]],\n",
    "                # windows_batch_size=60,\n",
    "                num_lr_decays=5,\n",
    "                # val_check_steps=20,\n",
    "                n_harmonics=0, n_polynomials=0,\n",
    "\n",
    "                max_steps=300,\n",
    "                # early_stop_patience_steps=2,\n",
    "\n",
    "                )\n",
    "  fcst = NeuralForecast(\n",
    "    models=[model],\n",
    "    freq='D'\n",
    "  )\n",
    "\n",
    "  fcst.fit(df=temp_df)\n",
    "  forecasts = fcst.predict()\n",
    "  return forecasts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_iKIk8Z14FH"
   },
   "outputs": [],
   "source": [
    "def nbeats_nextday_forecast(Data):\n",
    "  Data['unique_id'] = '1'\n",
    "  Data.rename(columns={'Date':'ds'}, inplace=True)\n",
    "  Data.rename(columns={'TrueSD':'y'}, inplace=True)\n",
    "\n",
    "  model = NBEATS(h=1, input_size=1,\n",
    "                loss=MQLoss(level=[90]),\n",
    "                learning_rate= 0.001,\n",
    "                stack_types=['identity', 'identity'],\n",
    "                n_blocks=[1,1],\n",
    "                mlp_units= [[512, 512], [512, 512]],\n",
    "                # windows_batch_size=60,\n",
    "                num_lr_decays=5,\n",
    "                # val_check_steps=20,\n",
    "                n_harmonics=0, n_polynomials=0,\n",
    "\n",
    "                max_steps=300,\n",
    "                # early_stop_patience_steps=2,\n",
    "\n",
    "                )\n",
    "  fcst = NeuralForecast(\n",
    "    models=[model],\n",
    "    freq='D'\n",
    "  )\n",
    "\n",
    "  fcst.fit(df=Data)\n",
    "  forecasts = fcst.predict()\n",
    "  return forecasts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utriuC_t14Cn"
   },
   "outputs": [],
   "source": [
    "#Fitting of Transformed ANN-ARCH model and forecasting of the next volatility value\n",
    "def T_ANN_ARCH_Fit (Data, df, Lag=1, LagSD=5, Timestep=10, Dropout=0.05, LearningRate=0.01, Epochs=10000, Alpha=0.005, DF=4, BatchSize=64):\n",
    "    #N-Beats Model is fitted\n",
    "    pred_nbeats = nbeats_model_forecasts(Data, df)\n",
    "    nbeats_nextday = nbeats_nextday_forecast(Data)\n",
    "    pred_nbeats.rename(columns={'ds': 'Date'}, inplace=True)\n",
    "    pred_nbeats = pred_nbeats.set_index('Date')\n",
    "    Data = Data.drop('ds', axis=1)\n",
    "    Data.rename(columns={'y': 'TrueSD'}, inplace=True)\n",
    "    Data = Data.drop('unique_id', axis=1)\n",
    "    # Database contaning N-Beats model is generated\n",
    "    Data_AR=pd.concat([Data, pred_nbeats['NBEATS-median']], axis=1)\n",
    "    if Data_AR.shape[0]!=Data.shape[0]: print(\"Error in DB Generation\")\n",
    "    #Original explanatory and response variables are generated\n",
    "    XData_AR = Data_AR.drop(Data_AR.columns[[0,2]], axis=1);YData_AR = Data_AR['TrueSD']\n",
    "    #Data is normalized\n",
    "    Scaled_Norm = preprocessing.StandardScaler().fit(XData_AR); XData_AR_Norm = Scaled_Norm.transform(XData_AR)\n",
    "    #Data for fitting the transformer model is generated\n",
    "    XData_AR_Norm_T, YData_AR_Norm_T= Transformer_Database(Timestep, XData_AR_Norm, YData_AR)\n",
    "    #Model with transformer layer is defined\n",
    "    model = Transformer_Model(XData_AR_Norm_T.shape[1], XData_AR_Norm_T.shape[2], HeadsAttention=4, Dropout=Dropout, LearningRate=LearningRate)\n",
    "    model.fit(XData_AR_Norm_T, YData_AR_Norm_T, epochs=Epochs, verbose=0, batch_size=BatchSize); tf.keras.backend.clear_session()\n",
    "    Forecast, Date_Forecast, TrainPrediction, ReturnForecast = T_ANN_ARCH_Forecast (Database, Lag, LagSD, nbeats_nextday['NBEATS-median'].values.item(0), Scaled_Norm, XData_AR, model)\n",
    "    VaR = T_ANN_ARCH_VaR(Alpha, Data['DailyReturnsOld'], Forecast,DF)\n",
    "    return {'T_ANN_ARCH_model':model, 'Forecast_T_ANN_ARCH':Forecast, 'Date_Forecast':Date_Forecast, 'TrainPrediction': TrainPrediction, 'Scaler':Scaled_Norm,  'ReturnForecast':ReturnForecast, 'Forecast_NBEATS': nbeats_nextday['NBEATS-median'].values.item(0)  , 'YData_Train':YData_AR_Norm_T, 'VaR': VaR}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6ugklEO25ku"
   },
   "outputs": [],
   "source": [
    "train_df = df.loc[:'2022-12-30'].reset_index(drop=False)\n",
    "test_df = df.loc['2022-12-30':].reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJD2BMgj27gk"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.set_index('Date')\n",
    "test_df = test_df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNgjcfQP29OL"
   },
   "outputs": [],
   "source": [
    "trIndexEndDays = train_df.index\n",
    "trIndexDates = trIndexEndDays[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qenQuDv92CnV"
   },
   "outputs": [],
   "source": [
    "Lag=1; LagSD=5; Timestep=10; Dropout=0.15; LearningRate=0.01; Epochs=50; Alpha=0.005; DF=4\n",
    "DataValidation = DatabaseGeneration_NBEATS(df, Lag, LagSD)\n",
    "DataValidation = DataValidation.resample('D').fillna(method='ffill')\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'Forecast_T_ANN_ARCH': [], 'ReturnForecast':[],'TrueSD':[], 'VaR_T_ANN_ARCH':[], 'Forecast_NBEATS':[]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xxZOBh6W2ChW",
    "outputId": "560ad1bc-508e-4120-b559-2cdd8f88f01e"
   },
   "outputs": [],
   "source": [
    "#Loop for generating the results\n",
    "for i in tqdm(range(trIndexDates.shape[0])):\n",
    "    df = df.resample('D').fillna(method='ffill')\n",
    "    Database=df.loc[(pd.to_datetime(df.index).date >= trIndexDates[i].date()-timedelta(days=650)) & (pd.to_datetime(df.index).date <= trIndexDates[i].date())]\n",
    "\n",
    "    Database = Database.resample('D').fillna(method='ffill')\n",
    "\n",
    "    if Database.index[-1] < pd.Timestamp(trIndexDates[i].date()-timedelta(days=1)):\n",
    "\n",
    "      new_date_range = pd.date_range(start=Database.index.min(), end= trIndexDates[i].date()-timedelta(days=1))\n",
    "\n",
    "      # Reindex the DataFrame to the new date range, forward-filling missing values\n",
    "      Database = Database.reindex(new_date_range).ffill()\n",
    "\n",
    "      # Reset the index if you want 'date' back as a column\n",
    "      Database.reset_index(inplace=True)\n",
    "      # Database = Database.drop('Date', axis=1)\n",
    "      Database.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "      Database = Database.set_index('Date')\n",
    "      Database.index = pd.to_datetime(Database.index)\n",
    "\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration_NBEATS(Database, Lag, LagSD)\n",
    "    Data = Data.resample('D').fillna(method='ffill')\n",
    "\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit (Data, df, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n",
    "\n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'Forecast_T_ANN_ARCH': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'],'Forecast_NBEATS':T_ANN_ARCH_Model['Forecast_NBEATS'],'ReturnForecast':T_ANN_ARCH_Model['ReturnForecast'],'TrueSD':DataValidation[DataValidation.index==pd.to_datetime(T_ANN_ARCH_Model['Date_Forecast'].date())]['TrueSD'][0], 'VaR_T_ANN_ARCH': T_ANN_ARCH_Model['VaR']}\n",
    "    ResultsCollection=ResultsCollection._append(IterResults, ignore_index=True)\n",
    "    # Results are saved\n",
    "    ResultsCollection.to_csv('Bajaj_Nbeats_Transformers_Train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qtnG00PBE3w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
